### Разверните S3 с использованием MinIO, Ceph или Object Storage от Yandex Cloud.
#### Установка на Debian 13
```bash
ksg@srv:~$ go install github.com/minio/minio@latest

ksg@new-srv:~$ go/bin/minio -v
minio version DEVELOPMENT.GOGET (commit-id=DEVELOPMENT.GOGET)
Runtime: go1.25.3 linux/amd64
License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
Copyright: 2015-0000 MinIO, Inc.

ksg@new-srv:~$ go/bin/minio server /mnt/data
INFO: Formatting 1st pool, 1 set(s), 1 drives per set.
INFO: WARNING: Host local has more than 0 drives of set. A host failure will result in data becoming unavailable.
MinIO Object Storage Server
Copyright: 2015-2025 MinIO, Inc.
License: GNU AGPLv3 - https://www.gnu.org/licenses/agpl-3.0.html
Version: DEVELOPMENT.GOGET (go1.25.3 linux/amd64)

API: http://192.168.10.112:9000  http://127.0.0.1:9000
   RootUser: minioadmin
   RootPass: minioadmin

WebUI: http://192.168.10.112:43987 http://127.0.0.1:43987
   RootUser: minioadmin
   RootPass: minioadmin

CLI: https://docs.min.io/community/minio-object-store/reference/minio-mc.html#quickstart
   $ mc alias set 'myminio' 'http://192.168.10.112:9000' 'minioadmin' 'minioadmin'

Docs: https://docs.min.io
```
#### установка minio из пакета для того, чтобы запустить службой
```bash
wget https://dl.min.io/aistor/minio/release/linux-amd64/minio_20251017061741.0.0_amd64.deb
dpkg -i minio_20251017061741.0.0_amd64.deb
```
#### замена исполняемого файла на `community` версию
```bash
sudo cp go/bin/minio /usr/local/bin/minio
```
#### конфигурационный файл
```bash
ksg@new-srv:~$ sudo cat /etc/default/minio
## Volume to be used for MinIO server.
MINIO_VOLUMES="/mnt/data"

## Use if you want to run MinIO on a custom port.
MINIO_OPTS="--address :9000 --console-address :9001"

## Root user for the server.
MINIO_ROOT_USER="root"

## Root secret for the server.
MINIO_ROOT_PASSWORD="00001111"
```

### Установите clickhouse-backup и настройте политику хранения (storage policy) в конфигурации ClickHouse.
Build from the sources (required go 1.21+):
```bash
ksg@click-01:~$ wget https://github.com/Altinity/clickhouse-backup/releases/download/v2.6.39/clickhouse-backup-linux-amd64.tar.gz
ksg@click-01:~$ tar -xvzf clickhouse-backup-linux-amd64.tar.gz
build/linux/amd64/clickhouse-backup
ksg@click-01:~$ sudo mv build/linux/amd64/clickhouse-backup /usr/local/bin/clickhouse-backup
ksg@click-01:~$ rm -rf build
ksg@click-01:~$ clickhouse-backup -v
Version:         2.6.39
Git Commit:      a94304a1549a38b05c32a3ba7665a285f3aba0de
Build Date:      2025-09-26
```

### Создайте тестовую базу данных с несколькими таблицами и заполните их данными.
```
click-01.lan :) create database s3test

Query id: 1ea4ea1d-3740-4b1f-aac7-60994e882fc9

Ok.

0 rows in set. Elapsed: 0.044 sec.
```
https://clickhouse.com/docs/ru/getting-started/example-datasets/menus
```bash
wget https://s3.amazonaws.com/menusdata.nypl.org/gzips/2021_08_01_07_01_17_data.tgz
tar xvf 2021_08_01_07_01_17_data.tgz
```
```sql
use s3test;
CREATE TABLE dish
(
    id UInt32,
    name String,
    description String,
    menus_appeared UInt32,
    times_appeared Int32,
    first_appeared UInt16,
    last_appeared UInt16,
    lowest_price Decimal64(3),
    highest_price Decimal64(3)
) ENGINE = MergeTree ORDER BY id;

CREATE TABLE menu
(
    id UInt32,
    name String,
    sponsor String,
    event String,
    venue String,
    place String,
    physical_description String,
    occasion String,
    notes String,
    call_number String,
    keywords String,
    language String,
    date String,
    location String,
    location_type String,
    currency String,
    currency_symbol String,
    status String,
    page_count UInt16,
    dish_count UInt16
) ENGINE = MergeTree ORDER BY id;

CREATE TABLE menu_page
(
    id UInt32,
    menu_id UInt32,
    page_number UInt16,
    image_id String,
    full_height UInt16,
    full_width UInt16,
    uuid UUID
) ENGINE = MergeTree ORDER BY id;

CREATE TABLE menu_item
(
    id UInt32,
    menu_page_id UInt32,
    price Decimal64(3),
    high_price Decimal64(3),
    dish_id UInt32,
    created_at DateTime,
    updated_at DateTime,
    xpos Float64,
    ypos Float64
) ENGINE = MergeTree ORDER BY id;
```
```bash
clickhouse-client -d s3test --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query "INSERT INTO dish FORMAT CSVWithNames" < Dish.csv
clickhouse-client -d s3test --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query "INSERT INTO menu FORMAT CSVWithNames" < Menu.csv
clickhouse-client -d s3test --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query "INSERT INTO menu_page FORMAT CSVWithNames" < MenuPage.csv
clickhouse-client -d s3test --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --date_time_input_format best_effort --query "INSERT INTO menu_item FORMAT CSVWithNames" < MenuItem.csv
```

### Выполните резервное копирование на удалённый ресурс (S3).
#### конфигурация `clickhouse-backup`
```bash
root@click-01:~# cat /etc/clickhouse-backup/config.yml
```
```yaml
general:
  remote_storage: s3           # REMOTE_STORAGE, choice from: `azblob`,`gcs`,`s3`, etc; if `none` then `upload` and `download` commands will fail.
#  max_file_size: 1073741824      # MAX_FILE_SIZE, 1G by default, useless when upload_by_part is true, use to split data parts files by archives
  backups_to_keep_local: 0       # BACKUPS_TO_KEEP_LOCAL, how many latest local backup should be kept, 0 means all created backups will be stored on local disk
                                 # -1 means backup will keep after `create` but will delete after `create_remote` command
                                 # You can run `clickhouse-backup delete local <backup_name>` command to remove temporary backup files from the local disk
  backups_to_keep_remote: 0      # BACKUPS_TO_KEEP_REMOTE, how many latest backup should be kept on remote storage, 0 means all uploaded backups will be stored on remote storage.
                                 # If old backups are required for newer incremental backup then it won't be deleted. Be careful with long incremental backup sequences.
  log_level: info                # LOG_LEVEL, a choice from `debug`, `info`, `warning`, `error`
  allow_empty_backups: false     # ALLOW_EMPTY_BACKUPS
  # Concurrency means parallel tables and parallel parts inside tables
  # For example, 4 means max 4 parallel tables and 4 parallel parts inside one table, so equals 16 concurrent streams
  download_concurrency: 1        # DOWNLOAD_CONCURRENCY, max 255, by default, the value is round(sqrt(AVAILABLE_CPU_CORES / 2))
  upload_concurrency: 1          # UPLOAD_CONCURRENCY, max 255, by default, the value is round(sqrt(AVAILABLE_CPU_CORES / 2))

  # Throttling speed for upload and download, calculates on part level, not the socket level, it means short period for high traffic values and then time to sleep
  download_max_bytes_per_second: 0  # DOWNLOAD_MAX_BYTES_PER_SECOND, 0 means no throttling
  upload_max_bytes_per_second: 0    # UPLOAD_MAX_BYTES_PER_SECOND, 0 means no throttling

  # when table data contains in system.disks with type=ObjectStorage, then we need execute remote copy object in object storage service provider, this parameter can restrict how many files will copied in parallel  for each table
  object_disk_server_side_copy_concurrency: 32
  # when CopyObject failure or object disk storage and backup destination have incompatible, will warning about possible high network traffic
  allow_object_disk_streaming: false

  # RESTORE_SCHEMA_ON_CLUSTER, execute all schema related SQL queries with `ON CLUSTER` clause as Distributed DDL.
  # Check `system.clusters` table for the correct cluster name, also `system.macros` can be used.
  # This isn't applicable when `use_embedded_backup_restore: true`
  restore_schema_on_cluster: ""
  upload_by_part: true           # UPLOAD_BY_PART
  download_by_part: true         # DOWNLOAD_BY_PART
  use_resumable_state: true      # USE_RESUMABLE_STATE, allow resume upload and download according to the <backup_name>.resumable file. Resumable state is not supported for custom method in remote storage.

  # RESTORE_DATABASE_MAPPING, restore rules from backup databases to target databases, which is useful when changing destination database, all atomic tables will be created with new UUIDs.
  # The format for this env variable is "src_db1:target_db1,src_db2:target_db2". For YAML please continue using map syntax
  restore_database_mapping: {}

  # RESTORE_TABLE_MAPPING, restore rules from backup tables to target tables, which is useful when changing destination tables.
  # The format for this env variable is "src_table1:target_table1,src_table2:target_table2". For YAML please continue using map syntax
  restore_table_mapping: {}

  retries_on_failure: 3          # RETRIES_ON_FAILURE, how many times to retry after a failure during upload or download
  retries_pause: 5s              # RETRIES_PAUSE, duration time to pause after each download or upload failure
  retries_jitter: 30             # RETRIES_JITTER, percent of RETRIES_PAUSE for jitter to avoid same time retries from parallel operations

  watch_interval: 1h       # WATCH_INTERVAL, use only for `watch` command, backup will create every 1h
  full_interval: 24h       # FULL_INTERVAL, use only for `watch` command, full backup will create every 24h
  watch_backup_name_template: "shard{shard}-{type}-{time:20060102150405}" # WATCH_BACKUP_NAME_TEMPLATE, used only for `watch` command, macros values will apply from `system.macros` for time:XXX, look format in https://go.dev/src/time/format.go

  sharded_operation_mode: none       # SHARDED_OPERATION_MODE, how different replicas will shard backing up data for tables. Options are: none (no sharding), table (table granularity), database (database granularity), first-replica (on the lexicographically sorted first active replica). If left empty, then the "none" option will be set as default.

  cpu_nice_priority: 15    # CPU niceness priority, to allow throttling CPU intensive operation, more details https://manpages.ubuntu.com/manpages/xenial/man1/nice.1.html
  io_nice_priority: "idle" # IO niceness priority, to allow throttling DISK intensive operation, more details https://manpages.ubuntu.com/manpages/xenial/man1/ionice.1.html

  rbac_backup_always: true # always backup RBAC objects
  rbac_resolve_conflicts: "recreate"  # action, when RBAC object with the same name already exists, allow "recreate", "ignore", "fail" values

  config_backup_always: false # always backup CONFIGS, disabled by default cause configuration shall be manage via Infrastructure as Code approach
  named_collections_backup_always: false # always backup Named Collections, disabled by default cause configuration shall be manage via Infrastructure as Code approach

clickhouse:
  username: default                # CLICKHOUSE_USERNAME
  password: "0000"                     # CLICKHOUSE_PASSWORD
  host: localhost                  # CLICKHOUSE_HOST, To make backup data `clickhouse-backup` requires access to the same file system as clickhouse-server, so `host` should localhost or address of another docker container on the same machine, or IP address bound to some network interface on the same host.
  port: 9000                       # CLICKHOUSE_PORT, don't use 8123, clickhouse-backup doesn't support HTTP protocol
  disk_mapping: {}
  skip_tables:
    - system.*
    - INFORMATION_SCHEMA.*
    - information_schema.*
  skip_table_engines: []
  skip_disks: []
  skip_disk_types: []
  timeout: 5m                  # CLICKHOUSE_TIMEOUT
  freeze_by_part: false        # CLICKHOUSE_FREEZE_BY_PART, allow freezing by part instead of freezing the whole table
  freeze_by_part_where: ""     # CLICKHOUSE_FREEZE_BY_PART_WHERE, allow parts filtering during freezing when freeze_by_part: true
  secure: false                # CLICKHOUSE_SECURE, use TLS encryption for connection
  skip_verify: false           # CLICKHOUSE_SKIP_VERIFY, skip certificate verification and allow potential certificate warnings
  sync_replicated_tables: true # CLICKHOUSE_SYNC_REPLICATED_TABLES
  tls_key: ""                  # CLICKHOUSE_TLS_KEY, filename with TLS key file
  tls_cert: ""                 # CLICKHOUSE_TLS_CERT, filename with TLS certificate file
  tls_ca: ""                   # CLICKHOUSE_TLS_CA, filename with TLS custom authority file
  log_sql_queries: true        # CLICKHOUSE_LOG_SQL_QUERIES, logging `clickhouse-backup` SQL queries on `info` level, when true, `debug` level when false
  debug: false                 # CLICKHOUSE_DEBUG
  config_dir:      "/etc/clickhouse-server"              # CLICKHOUSE_CONFIG_DIR
  restart_command: "exec:systemctl restart clickhouse-server"
  ignore_not_exists_error_during_freeze: true # CLICKHOUSE_IGNORE_NOT_EXISTS_ERROR_DURING_FREEZE, helps to avoid backup failures when running frequent CREATE / DROP tables and databases during backup, `clickhouse-backup` will ignore `code: 60` and `code: 81` errors during execution of `ALTER TABLE ... FREEZE`
  check_replicas_before_attach: true # CLICKHOUSE_CHECK_REPLICAS_BEFORE_ATTACH, helps avoiding concurrent ATTACH PART execution when restoring ReplicatedMergeTree tables
  default_replica_path: "/clickhouse/tables/{cluster}/{shard}/{database}/{table}" # CLICKHOUSE_DEFAULT_REPLICA_PATH, will use during restore Replicated tables without macros in replication_path if replica already exists, to avoid restoring conflicts
  default_replica_name: "{replica}" # CLICKHOUSE_DEFAULT_REPLICA_NAME, will use during restore Replicated tables without macros in replica_name if replica already exists, to avoid restoring conflicts
  use_embedded_backup_restore: false # CLICKHOUSE_USE_EMBEDDED_BACKUP_RESTORE, use BACKUP / RESTORE SQL statements instead of regular SQL queries to use features of modern ClickHouse server versions
  embedded_backup_disk: ""  # CLICKHOUSE_EMBEDDED_BACKUP_DISK - disk from system.disks which will use when `use_embedded_backup_restore: true`
  backup_mutations: true # CLICKHOUSE_BACKUP_MUTATIONS, allow backup mutations from system.mutations WHERE is_done=0 and apply it during restore
  restore_as_attach: false # CLICKHOUSE_RESTORE_AS_ATTACH, allow restore tables which have inconsistent data parts structure and mutations in progress
  restore_distributed_cluster: "" # CLICKHOUSE_RESTORE_DISTRIBUTED_CLUSTER, cluster name (can use macros) which will use during restore `engine=Distributed` tables, when cluster defined in backup table definition not exists in `system.clusters`
  check_parts_columns: true # CLICKHOUSE_CHECK_PARTS_COLUMNS, check data types from system.parts_columns during create backup to guarantee mutation is complete
  max_connections: 0 # CLICKHOUSE_MAX_CONNECTIONS, how many parallel connections could be opened during operations
s3:
  access_key: "root"                   # S3_ACCESS_KEY
  secret_key: "00001111"                   # S3_SECRET_KEY
  bucket: "testbucket"                       # S3_BUCKET
  endpoint: "http://192.168.10.112:9000"                     # S3_ENDPOINT
  region: us-east-1                # S3_REGION
  # AWS changed S3 defaults in April 2023 so that all new buckets have ACL disabled: https://aws.amazon.com/blogs/aws/heads-up-amazon-s3-security-changes-are-coming-in-april-of-2023/
  # They also recommend that ACLs are disabled: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ensure-object-ownership.html
  # use `acl: ""` if you see "api error AccessControlListNotSupported: The bucket does not allow ACLs"
  acl: private                     # S3_ACL
  assume_role_arn: ""              # S3_ASSUME_ROLE_ARN
  force_path_style: false          # S3_FORCE_PATH_STYLE
  path: ""                         # S3_PATH, `system.macros` values can be applied as {macro_name}
  object_disk_path: ""             # S3_OBJECT_DISK_PATH, path for backup of part from clickhouse object disks, if object disks present in clickhouse, then shall not be zero and shall not be prefixed by `path`
  disable_ssl: false               # S3_DISABLE_SSL
  compression_level: 1             # S3_COMPRESSION_LEVEL
  compression_format: tar          # S3_COMPRESSION_FORMAT, allowed values tar, lz4, bzip2, gzip, sz, xz, brortli, zstd, `none` for upload data part folders as is
  # look at details in https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html
  sse: ""                          # S3_SSE, empty (default), AES256, or aws:kms
  sse_customer_algorithm: ""       # S3_SSE_CUSTOMER_ALGORITHM, encryption algorithm, for example, AES256
  sse_customer_key: ""             # S3_SSE_CUSTOMER_KEY, customer-provided encryption key use `openssl rand 32 > aws_sse.key` and `cat aws_sse.key | base64`
  sse_customer_key_md5: ""         # S3_SSE_CUSTOMER_KEY_MD5, 128-bit MD5 digest of the encryption key according to RFC 1321 use `cat aws_sse.key |  openssl dgst -md5 -binary | base64`
  sse_kms_key_id: ""               # S3_SSE_KMS_KEY_ID, if S3_SSE is aws:kms then specifies the ID of the Amazon Web Services Key Management Service
  sse_kms_encryption_context: ""   # S3_SSE_KMS_ENCRYPTION_CONTEXT, base64-encoded UTF-8 string holding a JSON with the encryption context
                                   # Specifies the Amazon Web Services KMS Encryption Context to use for object encryption.
                                   # This is a collection of non-secret key-value pairs that represent additional authenticated data.
                                   # When you use an encryption context to encrypt data, you must specify the same (an exact case-sensitive match)
                                   # encryption context to decrypt the data. An encryption context is supported only on operations with symmetric encryption KMS keys
  disable_cert_verification: false # S3_DISABLE_CERT_VERIFICATION
  use_custom_storage_class: false  # S3_USE_CUSTOM_STORAGE_CLASS
  storage_class: STANDARD          # S3_STORAGE_CLASS, by default allow only from list https://github.com/aws/aws-sdk-go-v2/blob/main/service/s3/types/enums.go#L787-L799
  concurrency: 1                   # S3_CONCURRENCY
  chunk_size: 0                    # S3_CHUNK_SIZE, default 0: remoteSize / max_part_count
  max_parts_count: 4000            # S3_MAX_PARTS_COUNT, number of parts for S3 multipart uploads and downloads
  allow_multipart_download: false  # S3_ALLOW_MULTIPART_DOWNLOAD, allow faster multipart download speed, but will require additional disk space, download_concurrency * part size in worst case
  checksum_algorithm: ""           # S3_CHECKSUM_ALGORITHM, use it when you use object lock which allow to avoid delete keys from bucket until some timeout after creation, use CRC32 as fastest

  # S3_OBJECT_LABELS, allow setup metadata for each object during upload, use {macro_name} from system.macros and {backupName} for current backup name
  # The format for this env variable is "key1:value1,key2:value2". For YAML please continue using map syntax
  object_labels: {}
  # S3_CUSTOM_STORAGE_CLASS_MAP, allow setup storage class depending on the backup name regexp pattern, format nameRegexp > className
  custom_storage_class_map: {}
  # S3_REQUEST_PAYER, define who will pay to request, look https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html for details, possible values requester, if empty then bucket owner
  request_payer: ""
  debug: false                     # S3_DEBUG
```
#### создание бэкапа и копирование на s3 хранилище
```bash
root@click-01:~# clickhouse-backup create --tables=s3test.* s3backup
```
```log
2025-10-29 20:11:08.095 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:11:08.104 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:11:08.114 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` where name='VERSION_INTEGER'
2025-10-29 20:11:08.118 INF pkg/clickhouse/clickhouse.go:1186 > SELECT data_path AS metadata_path FROM system.databases WHERE name = 'system' LIMIT 1
2025-10-29 20:11:08.135 INF pkg/clickhouse/clickhouse.go:1186 > SELECT name, engine FROM system.databases WHERE NOT match(name,'^(system|INFORMATION_SCHEMA|information_schema|_temporary_and_external_tables)$') AND match(name,'^(s3test)$')
2025-10-29 20:11:08.179 INF pkg/clickhouse/clickhouse.go:1186 > SHOW CREATE DATABASE `s3test`
2025-10-29 20:11:08.187 INF pkg/clickhouse/clickhouse.go:1184 > SELECT name, count(*) as is_present FROM system.settings WHERE name IN (?, ?) GROUP BY name with args []interface {}{"show_table_uuid_in_table_create_query_if_not_nil", "display_secrets_in_show_and_select"}
2025-10-29 20:11:08.229 INF pkg/clickhouse/clickhouse.go:1186 > SELECT name FROM system.databases WHERE engine IN ('MySQL','PostgreSQL','MaterializedPostgreSQL')
2025-10-29 20:11:08.282 INF pkg/clickhouse/clickhouse.go:1186 >    SELECT     countIf(name='data_path') is_data_path_present,     countIf(name='data_paths') is_data_paths_present,     countIf(name='uuid') is_uuid_present,     countIf(name='create_table_query') is_create_table_query_present,     countIf(name='total_bytes') is_total_bytes_present    FROM system.columns WHERE database='system' AND table='tables'
2025-10-29 20:11:08.342 INF pkg/clickhouse/clickhouse.go:1186 > SELECT database, name, engine , data_paths , uuid , create_table_query , coalesce(total_bytes, 0) AS total_bytes   FROM system.tables WHERE is_temporary = 0 AND match(concat(database,'.',name),'^s3test\..*$')  ORDER BY total_bytes DESC SETTINGS show_table_uuid_in_table_create_query_if_not_nil=1
2025-10-29 20:11:08.781 INF pkg/clickhouse/clickhouse.go:1186 > SELECT data_path AS metadata_path FROM system.databases WHERE name = 'system' LIMIT 1
2025-10-29 20:11:08.804 INF pkg/clickhouse/clickhouse.go:1186 > SELECT count() as cnt FROM system.columns WHERE database='system' AND table='functions' AND name='create_query' SETTINGS empty_result_for_aggregation_by_empty_set=0
2025-10-29 20:11:08.922 INF pkg/clickhouse/clickhouse.go:1186 > SELECT name, create_query FROM system.functions WHERE create_query!=''
2025-10-29 20:11:09.023 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:11:09.089 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:11:09.113 INF pkg/clickhouse/clickhouse.go:1186 > SELECT name FROM system.user_directories WHERE type='replicated'
2025-10-29 20:11:09.148 INF pkg/clickhouse/clickhouse.go:1186 > SELECT JSONExtractString(params,'path') AS access_path FROM system.user_directories WHERE type in ('local_directory','local directory')
2025-10-29 20:11:09.252 INF pkg/backup/create.go:195 > done createBackupRBAC size=204B
2025-10-29 20:11:09.254 WRN pkg/backup/create.go:234 > 's3backup' medatata.json already exists, will overwrite and resume object disk data upload
2025-10-29 20:11:09.254 INF pkg/clickhouse/clickhouse.go:1184 > SELECT column, groupUniqArray(type) AS uniq_types FROM system.parts_columns WHERE active AND database=? AND table=? AND type NOT LIKE 'Enum%(%' AND type NOT LIKE 'Tuple(%' AND type NOT LIKE 'Nullable(Enum%(%' AND type NOT LIKE 'Nullable(Tuple(%' AND type NOT LIKE 'Array(Tuple(%' AND type NOT LIKE 'Nullable(Array(Tuple(%' GROUP BY column HAVING length(uniq_types) > 1 with args []interface {}{"s3test", "menu_item"}
2025-10-29 20:11:09.293 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu_item` FREEZE WITH NAME 'c2abe4c788094dd28152d899703819c3';
2025-10-29 20:11:09.395 WRN pkg/backup/create.go:928 > /var/lib/clickhouse/backup/s3backup/shadow/s3test/menu_item/default will clean to properly handle resume parameter
2025-10-29 20:11:09.402 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu_item` UNFREEZE WITH NAME 'c2abe4c788094dd28152d899703819c3'
2025-10-29 20:11:09.428 INF pkg/clickhouse/clickhouse.go:1184 > SELECT mutation_id, command FROM system.mutations WHERE is_done=0 AND database=? AND table=? with args []interface {}{"s3test", "menu_item"}
2025-10-29 20:11:09.451 INF pkg/backup/create.go:376 > done progress=1/4 table=s3test.menu_item
2025-10-29 20:11:09.455 INF pkg/clickhouse/clickhouse.go:1184 > SELECT column, groupUniqArray(type) AS uniq_types FROM system.parts_columns WHERE active AND database=? AND table=? AND type NOT LIKE 'Enum%(%' AND type NOT LIKE 'Tuple(%' AND type NOT LIKE 'Nullable(Enum%(%' AND type NOT LIKE 'Nullable(Tuple(%' AND type NOT LIKE 'Array(Tuple(%' AND type NOT LIKE 'Nullable(Array(Tuple(%' GROUP BY column HAVING length(uniq_types) > 1 with args []interface {}{"s3test", "dish"}
2025-10-29 20:11:09.555 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`dish` FREEZE WITH NAME '2183090a8ec04feaa8594f8e1cba728b';
2025-10-29 20:11:09.607 WRN pkg/backup/create.go:928 > /var/lib/clickhouse/backup/s3backup/shadow/s3test/dish/default will clean to properly handle resume parameter
2025-10-29 20:11:09.632 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`dish` UNFREEZE WITH NAME '2183090a8ec04feaa8594f8e1cba728b'
2025-10-29 20:11:09.659 INF pkg/clickhouse/clickhouse.go:1184 > SELECT mutation_id, command FROM system.mutations WHERE is_done=0 AND database=? AND table=? with args []interface {}{"s3test", "dish"}
2025-10-29 20:11:09.688 INF pkg/backup/create.go:376 > done progress=2/4 table=s3test.dish
2025-10-29 20:11:09.709 INF pkg/clickhouse/clickhouse.go:1184 > SELECT column, groupUniqArray(type) AS uniq_types FROM system.parts_columns WHERE active AND database=? AND table=? AND type NOT LIKE 'Enum%(%' AND type NOT LIKE 'Tuple(%' AND type NOT LIKE 'Nullable(Enum%(%' AND type NOT LIKE 'Nullable(Tuple(%' AND type NOT LIKE 'Array(Tuple(%' AND type NOT LIKE 'Nullable(Array(Tuple(%' GROUP BY column HAVING length(uniq_types) > 1 with args []interface {}{"s3test", "menu_page"}
2025-10-29 20:11:09.761 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu_page` FREEZE WITH NAME '2323b44ff9764bf1b91673ceec115c6e';
2025-10-29 20:11:09.790 WRN pkg/backup/create.go:928 > /var/lib/clickhouse/backup/s3backup/shadow/s3test/menu_page/default will clean to properly handle resume parameter
2025-10-29 20:11:09.803 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu_page` UNFREEZE WITH NAME '2323b44ff9764bf1b91673ceec115c6e'
2025-10-29 20:11:09.813 INF pkg/clickhouse/clickhouse.go:1184 > SELECT mutation_id, command FROM system.mutations WHERE is_done=0 AND database=? AND table=? with args []interface {}{"s3test", "menu_page"}
2025-10-29 20:11:09.865 INF pkg/backup/create.go:376 > done progress=3/4 table=s3test.menu_page
2025-10-29 20:11:09.868 INF pkg/clickhouse/clickhouse.go:1184 > SELECT column, groupUniqArray(type) AS uniq_types FROM system.parts_columns WHERE active AND database=? AND table=? AND type NOT LIKE 'Enum%(%' AND type NOT LIKE 'Tuple(%' AND type NOT LIKE 'Nullable(Enum%(%' AND type NOT LIKE 'Nullable(Tuple(%' AND type NOT LIKE 'Array(Tuple(%' AND type NOT LIKE 'Nullable(Array(Tuple(%' GROUP BY column HAVING length(uniq_types) > 1 with args []interface {}{"s3test", "menu"}
2025-10-29 20:11:09.952 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu` FREEZE WITH NAME '7512503eaae44ac5955dfe5a0eb33d6c';
2025-10-29 20:11:10.101 WRN pkg/backup/create.go:928 > /var/lib/clickhouse/backup/s3backup/shadow/s3test/menu/default will clean to properly handle resume parameter
2025-10-29 20:11:10.126 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu` UNFREEZE WITH NAME '7512503eaae44ac5955dfe5a0eb33d6c'
2025-10-29 20:11:10.151 INF pkg/clickhouse/clickhouse.go:1184 > SELECT mutation_id, command FROM system.mutations WHERE is_done=0 AND database=? AND table=? with args []interface {}{"s3test", "menu"}
2025-10-29 20:11:10.195 INF pkg/backup/create.go:376 > done progress=4/4 table=s3test.menu
2025-10-29 20:11:10.202 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` WHERE name='VERSION_DESCRIBE'
2025-10-29 20:11:10.303 INF pkg/backup/create.go:388 > done duration=2.208s operation=createBackupLocal version=2.6.39
2025-10-29 20:11:10.322 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
```
```bash
root@click-01:~# clickhouse-backup upload s3backup
```
```log
2025-10-29 20:13:02.899 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:13:02.936 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:13:02.936 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` where name='VERSION_INTEGER'
2025-10-29 20:13:02.949 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:13:03.025 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:13:03.096 INF pkg/clickhouse/clickhouse.go:1186 > SELECT max(toInt64(bytes_on_disk * 1.02)) AS max_file_size FROM system.parts WHERE active SETTINGS empty_result_for_aggregation_by_empty_set=0
2025-10-29 20:13:03.157 INF pkg/clickhouse/clickhouse.go:1186 > SELECT count() AS is_macros_exists FROM system.tables WHERE database='system' AND name='macros'  SETTINGS empty_result_for_aggregation_by_empty_set=0
2025-10-29 20:13:03.186 INF pkg/clickhouse/clickhouse.go:1186 > SELECT macro, substitution FROM system.macros
2025-10-29 20:13:03.229 INF pkg/clickhouse/clickhouse.go:1186 > SELECT count() AS is_macros_exists FROM system.tables WHERE database='system' AND name='macros'  SETTINGS empty_result_for_aggregation_by_empty_set=0
2025-10-29 20:13:03.271 INF pkg/clickhouse/clickhouse.go:1186 > SELECT macro, substitution FROM system.macros
2025-10-29 20:13:03.332 INF pkg/storage/general.go:164 > list_duration=7.460617
2025-10-29 20:13:03.394 INF pkg/resumable/state.go:101 > parameters changed old=map[string]interface {}{"diffFrom":"", "diffFromRemote":"", "partitions":[]interface {}{}, "schemaOnly":false, "tablePattern":"s3test.*"} new=map[string]interface {}{"diffFrom":"", "diffFromRemote":"", "partitions":[]string{}, "schemaOnly":false, "tablePattern":""}, /var/lib/clickhouse/backup/s3backup/upload.state2 cleanup begin
2025-10-29 20:13:05.411 INF pkg/backup/upload.go:191 > done data_size=12.41MiB duration=1.904s metadata_size=725B operation=upload_table progress=1/4 table=s3test.dish version=2.6.39
2025-10-29 20:13:05.711 INF pkg/backup/upload.go:191 > done data_size=1.06MiB duration=2.205s metadata_size=895B operation=upload_table progress=2/4 table=s3test.menu version=2.6.39
2025-10-29 20:13:10.274 INF pkg/backup/upload.go:191 > done data_size=36.77MiB duration=4.863s metadata_size=809B operation=upload_table progress=3/4 table=s3test.menu_item version=2.6.39
2025-10-29 20:13:10.755 INF pkg/backup/upload.go:191 > done data_size=1.20MiB duration=5.035s metadata_size=650B operation=upload_table progress=4/4 table=s3test.menu_page version=2.6.39
2025-10-29 20:13:10.950 INF pkg/backup/upload.go:269 > done backup=s3backup duration=8.052s object_disk_size=0B operation=upload upload_size=51.45MiB version=2.6.39
2025-10-29 20:13:10.951 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
```

### Повредите данные (удалите таблицу, измените строки и т.д.).
```sql
click-01.lan :) drop table s3test.menu_page no delay

Query id: 9770564a-09d9-489b-a304-6c0d0768e433

Ok.

0 rows in set. Elapsed: 0.165 sec.
```

### Восстановите данные из резервной копии.
```bash
root@click-01:~# clickhouse-backup restore_remote --table=s3test.menu_page s3backup
```
```log
2025-10-29 20:18:54.340 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:18:54.368 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:18:54.377 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` where name='VERSION_INTEGER'
2025-10-29 20:18:54.458 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:18:54.486 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:18:54.592 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
2025-10-29 20:18:54.593 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:18:54.602 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:18:54.602 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:18:54.650 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:18:54.675 INF pkg/clickhouse/clickhouse.go:1186 > DROP TABLE IF EXISTS `s3test`.`menu_page` NO DELAY
2025-10-29 20:18:54.683 INF pkg/clickhouse/clickhouse.go:1186 > CREATE DATABASE IF NOT EXISTS `s3test` ENGINE=Atomic
2025-10-29 20:18:54.685 INF pkg/clickhouse/clickhouse.go:1186 > CREATE TABLE s3test.menu_page UUID '904b77aa-9a88-48ee-8d85-62c30b6d0609' (`id` UInt32, `menu_id` UInt32, `page_number` UInt16, `image_id` String, `full_height` UInt16, `full_width` UInt16, `uuid` UUID) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192
2025-10-29 20:18:54.688 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
2025-10-29 20:18:54.689 FTL cmd/clickhouse-backup/main.go:823 > error="can't create table `s3test`.`menu_page`: code: 57, message: Directory for table data store/904/904b77aa-9a88-48ee-8d85-62c30b6d0609/ already exists after 1 times, please check your schema dependencies"
root@click-01:~#
root@click-01:~# clickhouse-backup restore_remote --table=s3test.menu_page s3backup
2025-10-29 20:19:49.394 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:19:49.399 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:19:49.399 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` where name='VERSION_INTEGER'
2025-10-29 20:19:49.405 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:19:49.461 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:19:49.575 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
2025-10-29 20:19:49.595 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:19:49.619 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:19:49.619 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:19:49.672 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:19:49.772 INF pkg/clickhouse/clickhouse.go:1186 > DROP TABLE IF EXISTS `s3test`.`menu_page` NO DELAY
2025-10-29 20:19:49.792 INF pkg/clickhouse/clickhouse.go:1186 > CREATE DATABASE IF NOT EXISTS `s3test` ENGINE=Atomic
2025-10-29 20:19:49.794 INF pkg/clickhouse/clickhouse.go:1186 > CREATE TABLE s3test.menu_page UUID '904b77aa-9a88-48ee-8d85-62c30b6d0609' (`id` UInt32, `menu_id` UInt32, `page_number` UInt16, `image_id` String, `full_height` UInt16, `full_width` UInt16, `uuid` UUID) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192
2025-10-29 20:19:49.798 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
2025-10-29 20:19:49.801 FTL cmd/clickhouse-backup/main.go:823 > error="can't create table `s3test`.`menu_page`: code: 57, message: Directory for table data store/904/904b77aa-9a88-48ee-8d85-62c30b6d0609/ already exists after 1 times, please check your schema dependencies"
root@click-01:~#
root@click-01:~# clickhouse-backup restore_remote --table=s3test.menu_page s3backup
2025-10-29 20:28:33.431 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:28:33.436 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:28:33.438 INF pkg/clickhouse/clickhouse.go:1186 > SELECT value FROM `system`.`build_options` where name='VERSION_INTEGER'
2025-10-29 20:28:33.443 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:28:33.459 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:28:33.466 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
2025-10-29 20:28:33.466 INF pkg/clickhouse/clickhouse.go:129 > clickhouse connection prepared: tcp://localhost:9000 run ping
2025-10-29 20:28:33.471 INF pkg/clickhouse/clickhouse.go:132 > clickhouse connection success: tcp://localhost:9000
2025-10-29 20:28:33.473 INF pkg/clickhouse/clickhouse.go:1186 > SELECT countIf(name='type') AS is_disk_type_present, countIf(name='object_storage_type') AS is_object_storage_type_present, countIf(name='free_space') AS is_free_space_present, countIf(name='disks') AS is_storage_policy_present FROM system.columns WHERE database='system' AND table IN ('disks','storage_policies')
2025-10-29 20:28:33.479 INF pkg/clickhouse/clickhouse.go:1186 > SELECT d.path AS path, any(d.name) AS name, any(lower(if(d.type='ObjectStorage',d.object_storage_type,d.type))) AS type, min(d.free_space) AS free_space, groupUniqArray(s.policy_name) AS storage_policies FROM system.disks AS d  LEFT JOIN (SELECT policy_name, arrayJoin(disks) AS disk FROM system.storage_policies) AS s ON s.disk = d.name GROUP BY d.path
2025-10-29 20:28:33.489 INF pkg/clickhouse/clickhouse.go:1186 > DROP TABLE IF EXISTS `s3test`.`menu_page` NO DELAY
2025-10-29 20:28:33.495 INF pkg/clickhouse/clickhouse.go:1186 > CREATE DATABASE IF NOT EXISTS `s3test` ENGINE=Atomic
2025-10-29 20:28:33.497 INF pkg/clickhouse/clickhouse.go:1186 > CREATE TABLE s3test.menu_page UUID '904b77aa-9a88-48ee-8d85-62c30b6d0609' (`id` UInt32, `menu_id` UInt32, `page_number` UInt16, `image_id` String, `full_height` UInt16, `full_width` UInt16, `uuid` UUID) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192
2025-10-29 20:28:33.584 INF pkg/backup/restore.go:1402 > done backup=s3backup duration=96ms operation=restore_schema
2025-10-29 20:28:33.585 INF pkg/clickhouse/clickhouse.go:1186 > SELECT count() AS is_macros_exists FROM system.tables WHERE database='system' AND name='macros'  SETTINGS empty_result_for_aggregation_by_empty_set=0
2025-10-29 20:28:33.609 INF pkg/clickhouse/clickhouse.go:1186 > SELECT macro, substitution FROM system.macros
2025-10-29 20:28:33.615 INF pkg/clickhouse/clickhouse.go:1184 > SELECT name, count(*) as is_present FROM system.settings WHERE name IN (?, ?) GROUP BY name with args []interface {}{"show_table_uuid_in_table_create_query_if_not_nil", "display_secrets_in_show_and_select"}
2025-10-29 20:28:33.644 INF pkg/clickhouse/clickhouse.go:1186 > SELECT name FROM system.databases WHERE engine IN ('MySQL','PostgreSQL','MaterializedPostgreSQL')
2025-10-29 20:28:33.650 INF pkg/clickhouse/clickhouse.go:1186 >    SELECT     countIf(name='data_path') is_data_path_present,     countIf(name='data_paths') is_data_paths_present,     countIf(name='uuid') is_uuid_present,     countIf(name='create_table_query') is_create_table_query_present,     countIf(name='total_bytes') is_total_bytes_present    FROM system.columns WHERE database='system' AND table='tables'
2025-10-29 20:28:33.697 INF pkg/clickhouse/clickhouse.go:1186 > SELECT database, name, engine , data_paths , uuid , create_table_query , coalesce(total_bytes, 0) AS total_bytes   FROM system.tables WHERE is_temporary = 0 AND match(concat(database,'.',name),'^s3test\.menu_page$')  ORDER BY total_bytes DESC SETTINGS show_table_uuid_in_table_create_query_if_not_nil=1
2025-10-29 20:28:33.954 INF pkg/clickhouse/clickhouse.go:1186 > SELECT data_path AS metadata_path FROM system.databases WHERE name = 'system' LIMIT 1
2025-10-29 20:28:33.974 INF pkg/clickhouse/clickhouse.go:1186 > SELECT sum(bytes_on_disk) as size FROM system.parts WHERE active AND database='s3test' AND table='menu_page' GROUP BY database, table
2025-10-29 20:28:34.050 INF pkg/backup/restore.go:1967 > download object_disks start table=s3test.menu_page
2025-10-29 20:28:34.068 INF pkg/backup/restore.go:1974 > download object_disks finish database=s3test duration=0s size=0B table=menu_page
2025-10-29 20:28:34.068 INF pkg/clickhouse/clickhouse.go:1186 > ALTER TABLE `s3test`.`menu_page` ATTACH PART 'all_1_1_0'
2025-10-29 20:28:34.090 INF pkg/backup/restore.go:1927 > done database=s3test duration=40ms operation=restoreDataRegular progress=1/1 table=menu_page
2025-10-29 20:28:34.090 INF pkg/backup/restore.go:1839 > done backup=s3backup duration=506ms operation=restore_data
2025-10-29 20:28:34.090 INF pkg/backup/restore.go:276 > done duration=624ms operation=restore version=2.6.39
2025-10-29 20:28:34.090 INF pkg/clickhouse/clickhouse.go:334 > clickhouse connection closed
```

### Убедитесь, что повреждённые данные успешно восстановлены.
```sql
click-01.lan :) select count(*) from s3test.menu_page

SELECT count(*)
FROM menu_page

Query id: bc3d2ef1-e666-4e07-91ab-3e866a675c4e

   ┌─count()─┐
1. │   66937 │
   └─────────┘

1 row in set. Elapsed: 0.016 sec.
```
